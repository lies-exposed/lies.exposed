parameters:
    model: Qwen3-4B.Q4_K_M.gguf
    language: ""
    translate: false
    "n": 1
    top_p: 0.9
    top_k: 40
    temperature: 0.7
    max_tokens: 2048
    echo: false
    batch: 1
    ignore_eos: false
    repeat_penalty: 0
    repeat_last_n: 0
    n_keep: 0
    frequency_penalty: 0
    presence_penalty: 0
    tfz: 1
    typical_p: 1
    seed: -1
    negative_prompt: ""
    rope_freq_base: 0
    rope_freq_scale: 0
    negative_prompt_scale: 0
    clip_skip: 0
    tokenizer: ""
name: qwen3-4b
f16: true
threads: 4
debug: true
roles: {}
embeddings: false
backend: llama-cpp
known_usecases:
    - FLAG_ANY
    - FLAG_COMPLETION
    - FLAG_CHAT
pipeline:
    tts: ""
    llm: ""
    transcription: ""
    vad: ""

function:
  disable_no_action: false

  grammar:
    disable: true

  return_name_in_function_response: true
  json_regex_match: 
   - "(?s)<tool_call>\\s*\\[?\\s*(\\{.*?\\})\\s*\\]?\\s*</tool_call>"
   - "(?s)<tool_call>\\s*\\[?\\s*(\\{.*?)\\s*\\]?\\s*$"
  replace_llm_results:
  - key: "(?s)<think>.*?</think>"
    value: ""
  - key: "(?s)<scratchpad>.*?</scratchpad>"
    value: ""
  - key: "^\\n+"
    value: ""
  - key: "(?m)^\\s*\\(\\)\\s*$"
    value: ""
  - key: "\\s*\\(\\)\\s*"
    value: " "

template:
  chat: |
    {{.Input -}}
    <|im_start|>assistant
  chat_message: |
    <|im_start|>{{if eq .RoleName "assistant"}}assistant{{else if eq .RoleName "system"}}system{{else if eq .RoleName "tool"}}tool{{else if eq .RoleName "user"}}user{{end}}
    {{- if .FunctionCall }}
    <tool_call>
    {{- else if eq .RoleName "tool" }}
    <tool_response>
    {{- end }}
    {{- if .Content}}
    {{.Content }}
    {{- end }}
    {{- if .FunctionCall}}
    {{toJson .FunctionCall}}
    {{- end }}
    {{- if .FunctionCall }}
    </tool_call>
    {{- else if eq .RoleName "tool" }}
    </tool_response>
    {{- end }}<|im_end|>
  completion: |
    {{.Input}}
  function: |-
    <|im_start|>system
    You are a helpful assistant with access to tools. You can call tools and then provide a final response based on the results.

    Available tools:
    <tools>
    {{range .Functions}}
    {'type': 'function', 'function': {'name': '{{.Name}}', 'description': '{{.Description}}', 'parameters': {{toJson .Parameters}} }}
    {{end}}
    </tools>

    Instructions:
    1. If you need information from tools, call them first using this format:
       <tool_call>
       {"arguments": <args-dict>, "name": <function-name>}
       </tool_call>
    2. After receiving tool results, provide a helpful response based on the information.
    3. If no tools are needed, respond directly.
    4. Always end with a complete response to help the user.<|im_end|>
    {{.Input -}}
    <|im_start|>assistant
feature_flags: {}
system_prompt: ""
tensor_split: ""
main_gpu: ""
rms_norm_eps: 0
ngqa: 0
prompt_cache_path: ""
prompt_cache_all: false
prompt_cache_ro: false
mirostat_eta: 0.1
mirostat_tau: 5
mirostat: 0
gpu_layers: 256
mmap: false
mmlock: false
low_vram: false
reranking: false
grammar: ""
stopwords:
    - <|im_end|>
    - <dummy32000>
    - </s>
    - <|endoftext|>
cutstrings: []
extract_regex: []
trimspace: []
trimsuffix: []
context_size: 8192
numa: false
request_timeout: 300
response_timeout: 300
lora_adapter: ""
lora_base: ""
lora_adapters: []
lora_scales: []
lora_scale: 0
no_mulmatq: false
draft_model: ""
n_draft: 0
quantization: ""
load_format: ""
gpu_memory_utilization: 0
trust_remote_code: false
enforce_eager: false
swap_space: 0
max_model_len: 0
tensor_parallel_size: 0
disable_log_stats: false
dtype: ""
limit_mm_per_prompt:
    image: 0
    video: 0
    audio: 0
mmproj: ""
flash_attention: null
no_kv_offloading: false
cache_type_k: ""
cache_type_v: ""
rope_scaling: ""
type: ""
yarn_ext_factor: 0
yarn_attn_factor: 0
yarn_beta_fast: 0
yarn_beta_slow: 0
cfg_scale: 0
diffusers:
    cuda: false
    pipeline_type: ""
    scheduler_type: ""
    enable_parameters: ""
    img2img: false
    clip_skip: 0
    clip_model: ""
    clip_subfolder: ""
    control_net: ""
step: 0
grpc:
    attempts: 0
    attempts_sleep_time: 0
tts:
    voice: ""
    audio_path: ""
cuda: false
download_files: []
description: ""
usage: ""
options: []
overrides: []
