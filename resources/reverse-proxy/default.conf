client_max_body_size 2G;

# Streaming endpoints need longer timeouts for LLM inference
# These timeouts apply to proxied requests (chat streaming, file uploads, etc.)
proxy_connect_timeout 30s;
proxy_send_timeout 300s;        # 5 minutes for sending request body
proxy_read_timeout 300s;        # 5 minutes for reading response
proxy_buffering off;            # Disable buffering for streaming responses
proxy_request_buffering off;    # Disable request buffering